{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743f48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, os, sys, copy, argparse\n",
    "import multiprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b727c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "from moviepy.video.tools.subtitles import SubtitlesClip\n",
    "\n",
    "from pathlib import Path\n",
    "import librosa, librosa.display\n",
    "import resampy\n",
    "\n",
    "import soundfile as sf \n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "747093ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "\n",
    "from model import BlazeNet \n",
    "from dataset import AudioDataset\n",
    "from spectrogram import generate_log_spectrogram\n",
    "from video_utils import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca286e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'output/'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6e42be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    #transforms.Resize(size=128),\n",
    "    #transforms.CenterCrop(size=224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a28f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlazeNet(\n",
       "  (backbone1): Sequential(\n",
       "    (0): Conv2d(3, 24, kernel_size=(5, 5), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
       "        (1): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24)\n",
       "        (1): Conv2d(24, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(28, 28, kernel_size=(3, 3), stride=(2, 2), groups=28)\n",
       "        (1): Conv2d(28, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
       "        (1): Conv2d(32, 36, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36)\n",
       "        (1): Conv2d(36, 42, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(42, 42, kernel_size=(3, 3), stride=(2, 2), groups=42)\n",
       "        (1): Conv2d(42, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=48)\n",
       "        (1): Conv2d(48, 56, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56)\n",
       "        (1): Conv2d(56, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
       "        (1): Conv2d(64, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72)\n",
       "        (1): Conv2d(72, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80)\n",
       "        (1): Conv2d(80, 88, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone2): Sequential(\n",
       "    (0): BlazeBlock(\n",
       "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(2, 2), groups=88)\n",
       "        (1): Conv2d(88, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): BlazeBlock(\n",
       "      (convs): Sequential(\n",
       "        (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "        (1): Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (act): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_8): Conv2d(88, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (classifier_16): Conv2d(96, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=224, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a neural network model \n",
    "model_ft = BlazeNet(back_model=2)\n",
    "model_ft = torch.load(\"checkpoints/blazenet_trainset_8000_4.1_None_64_0_None_512_512.pk.pth\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ft = model_ft.to(device)\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ce01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=8000; segment=4.1; pre_emphasis=None\n",
    "n_mels=64; fmin=0; fmax=None\n",
    "n_fft=512; hop_length=512\n",
    "VAD_on = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9393b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cry_detection_on_video(video_file,sr=8000,segment=4.1,pre_emphasis=None,\n",
    "                                n_mels=64,fmin=0,fmax=None,n_fft=512,hop_length=512,\n",
    "                                VAD_on=True,verbose=False):\n",
    "    \n",
    "    video_file_annot = video_file +'_crydetected.MP4'\n",
    "    if os.path.exists(video_file_annot):\n",
    "        return 0 \n",
    "    \n",
    "    \n",
    "    print(\"Step 1.1: Extracting audio file from video file {}\".format(video_file))\n",
    "    audio_file = extract_audio(video_file) \n",
    "    \n",
    "    print(\"Step 1.2: Read audio file {} and resample to {}\".format(audio_file,sr))     \n",
    "    audio_data, sr = librosa.load(audio_file,sr=sr)\n",
    "    \n",
    "    if verbose:        \n",
    "        duration = librosa.get_duration(y=wav_data, sr=sr)    \n",
    "        print(\"sampling rate = {}, length = {}, durations ={}s\".format(sr,len(audio_data),duration))        \n",
    "        plt.figure(1)\n",
    "        plt.title(\"Signal Wave...\")\n",
    "        plt.plot(audio_data)\n",
    "        plt.show()  \n",
    "        \n",
    "    print(\"Step 2.1: Generating log-mel-spectrogram\")\n",
    "    melgrams = generate_log_spectrogram(audio_file,None,sr=sr,duration=None,segment=segment,pre_emphasis=pre_emphasis,\n",
    "                                    n_mels=n_mels,fmin=fmin,fmax=fmax,                                     \n",
    "                                    n_fft=n_fft, hop_length=hop_length,VAD=VAD_on,debug=True)\n",
    "    \n",
    "    if len(melgrams)>0 and verbose: \n",
    "        print(len(melgrams))\n",
    "        for m in melgrams:\n",
    "            if m[2]:\n",
    "                print(m[0].shape,m[1],m[2])\n",
    "                break\n",
    "                    \n",
    "\n",
    "    print(\"Step 2.2 Saving into test dataset\")\n",
    "    test_data = {\"test\": {\"cry\":[],\"nocry\":[]}\n",
    "                }\n",
    "    test_data[\"test\"][\"cry\"].append((audio_file,melgrams))\n",
    "    dataset_file = output_dir+'testset_{}_{}_{}_{}_{}_{}_{}_{}_{}.pkl'.format(audio_file.split('/')[-1],\n",
    "                                                                             sr,segment,pre_emphasis,n_mels,\n",
    "                                                                             fmin,fmax,n_fft,hop_length)     \n",
    "    \n",
    "    print(dataset_file)\n",
    "    with open(dataset_file , 'wb') as pk_file:\n",
    "        pickle.dump(test_data, pk_file)        \n",
    "    \n",
    "    test_dataset2 = AudioDataset(dataset_file,\n",
    "                                 subset=\"test\",\n",
    "                                 mode = \"RGB\",\n",
    "                                 transform = test_transform)\n",
    "    test_loader2 = data.DataLoader(test_dataset2,\n",
    "                                   batch_size=32,\n",
    "                                   shuffle=False,\n",
    "                                   num_workers=4)\n",
    "\n",
    "    if verbose: \n",
    "        for i, (img,label,src,ind,vocal) in enumerate(test_loader2):\n",
    "            print(i)\n",
    "            print(img.shape)\n",
    "            print(label)\n",
    "            print(src)\n",
    "            print(ind)\n",
    "            print(vocal)\n",
    "            break  \n",
    "            \n",
    "    if len(test_dataset2)==0:\n",
    "        print(\"Found no cry in video \",video_file)\n",
    "        return 1 \n",
    "    \n",
    "    print(\"Step 3: Running cry detection CNN model\")\n",
    "    predictions = [] \n",
    "    with torch.no_grad():\n",
    "        for images, labels, srcs, inds, vocals in test_loader2:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model_ft(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            probs = torch.softmax(outputs.data, 1)[:,0]\n",
    "            \n",
    "            probs = probs.cpu().detach().numpy()\n",
    "            inds = inds.detach().numpy()\n",
    "            vocals = vocals.detach().numpy()\n",
    "\n",
    "            if verbose:\n",
    "                print(srcs)\n",
    "                print(probs)\n",
    "                print(inds)\n",
    "                print(vocals)        \n",
    "                #print(predicted)\n",
    "\n",
    "            pred = predicted.tolist()       \n",
    "            \n",
    "            for k in range(len(srcs)):\n",
    "                predictions.append((srcs[k],probs[k],inds[k],vocals[k])) # 1 is cry \n",
    "                \n",
    "\n",
    "    predictions_by_audio = {} \n",
    "    cry_thresh = 0.5 \n",
    "    audio_files = [x[0] for x in predictions]\n",
    "    audio_files = list(set(audio_files))\n",
    "    \n",
    "    for au in audio_files:\n",
    "        predictions_by_audio[au] = [] \n",
    "        \n",
    "    for p in predictions: \n",
    "        predictions_by_audio[p[0]].append((p[1],p[2],p[3],p[1]>cry_thresh))\n",
    "    for au in predictions_by_audio:\n",
    "        print(au)\n",
    "        print(predictions_by_audio[au])\n",
    "        pred = predictions_by_audio[au]\n",
    "\n",
    "    print(\"Step 4: Annotating the video file\")    \n",
    "    subs = [] \n",
    "    step = np.ceil(segment)\n",
    "    for i in range(len(pred)):    \n",
    "        s = pred[i][1]*step\n",
    "        t = (pred[i][1]+1)*step\n",
    "        if pred[i][3]:\n",
    "            subs.append(((s,t),'Cry'))\n",
    "        #else:\n",
    "        #    subs.append(((s,t),'nocry'))\n",
    "    print(subs)\n",
    "    generator = lambda txt: TextClip(txt, font='Arial', fontsize=48, color='red')\n",
    "    subtitles = SubtitlesClip(subs, generator)\n",
    "    \n",
    "    video = VideoFileClip(video_file)\n",
    "    result = CompositeVideoClip([video, subtitles.set_pos(('center','bottom'))])    \n",
    "    result.write_videofile(video_file_annot, fps=video.fps, remove_temp=True, codec=\"libx264\", audio_codec=\"aac\")\n",
    "    \n",
    "    return 2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9709a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E062904F3842_subjectawake_1637580472817.mp4', 'E06290646B49_subjectAsleep_1638305984190.mp4', 'E062904F3842_subjectawake_1637717631593.mp4', 'E062906A5A6D_subjectawake_1638712785921.mp4', 'E06290634162_subjectawake_1638677765467.mp4', '146B9CB78A18_subjectawake_1620132682993.mp4', '146B9CB78294_subjectawake_1620198310758.mp4', '146B9C6FA25A_monitoringOff_1619739616502.mp4', 'E06290637154_breathingStopped_1637525474363.mp4', '7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4']\n"
     ]
    }
   ],
   "source": [
    "#video_file = 'Self/10Cry/E062904F3842_subjectawake_1637717631593.mp4'\n",
    "video_dir = 'Self/10Cry/'\n",
    "video_files = os.listdir(video_dir)\n",
    "video_files = [x for x in video_files if (x.endswith('mp4') or x.endswith('MP4')) and not 'crydetected' in x]\n",
    "print(video_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfaf5484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1.1: Extracting audio file from video file Self/10Cry/E06290634162_subjectawake_1638677765467.mp4\n",
      "video_file =  Self/10Cry/E06290634162_subjectawake_1638677765467.mp4\n",
      "Self/10Cry/E06290634162_subjectawake_1638677765467.mp3  already exists\n",
      "Step 1.2: Read audio file Self/10Cry/E06290634162_subjectawake_1638677765467.mp3 and resample to 8000\n",
      "Step 2.1: Generating log-mel-spectrogram\n",
      "Self/10Cry/E06290634162_subjectawake_1638677765467.mp3 E06290634162_subjectawake_1638677765467.mp3\n",
      "311.41 4.1 5.0 62\n",
      "0 0.0 False (64, 64)\n",
      "1 5.0 False (64, 64)\n",
      "2 10.0 False (64, 64)\n",
      "3 15.0 False (64, 64)\n",
      "4 20.0 False (64, 64)\n",
      "5 25.0 False (64, 64)\n",
      "6 30.0 False (64, 64)\n",
      "7 35.0 False (64, 64)\n",
      "8 40.0 False (64, 64)\n",
      "9 45.0 False (64, 64)\n",
      "10 50.0 False (64, 64)\n",
      "11 55.0 False (64, 64)\n",
      "12 60.0 False (64, 64)\n",
      "13 65.0 False (64, 64)\n",
      "14 70.0 False (64, 64)\n",
      "15 75.0 False (64, 64)\n",
      "16 80.0 False (64, 64)\n",
      "17 85.0 False (64, 64)\n",
      "18 90.0 False (64, 64)\n",
      "19 95.0 False (64, 64)\n",
      "20 100.0 False (64, 64)\n",
      "21 105.0 False (64, 64)\n",
      "22 110.0 False (64, 64)\n",
      "23 115.0 False (64, 64)\n",
      "24 120.0 False (64, 64)\n",
      "25 125.0 False (64, 64)\n",
      "26 130.0 False (64, 64)\n",
      "27 135.0 False (64, 64)\n",
      "28 140.0 False (64, 64)\n",
      "29 145.0 False (64, 64)\n",
      "30 150.0 False (64, 64)\n",
      "31 155.0 False (64, 64)\n",
      "32 160.0 False (64, 64)\n",
      "33 165.0 False (64, 64)\n",
      "34 170.0 False (64, 64)\n",
      "35 175.0 False (64, 64)\n",
      "36 180.0 False (64, 64)\n",
      "37 185.0 False (64, 64)\n",
      "38 190.0 False (64, 64)\n",
      "39 195.0 False (64, 64)\n",
      "40 200.0 False (64, 64)\n",
      "41 205.0 False (64, 64)\n",
      "42 210.0 False (64, 64)\n",
      "43 215.0 False (64, 64)\n",
      "44 220.0 False (64, 64)\n",
      "45 225.0 False (64, 64)\n",
      "46 230.0 False (64, 64)\n",
      "47 235.0 False (64, 64)\n",
      "48 240.0 False (64, 64)\n",
      "49 245.0 False (64, 64)\n",
      "50 250.0 False (64, 64)\n",
      "51 255.0 False (64, 64)\n",
      "52 260.0 False (64, 64)\n",
      "53 265.0 False (64, 64)\n",
      "54 270.0 False (64, 64)\n",
      "55 275.0 False (64, 64)\n",
      "56 280.0 False (64, 64)\n",
      "57 285.0 False (64, 64)\n",
      "58 290.0 False (64, 64)\n",
      "59 295.0 False (64, 64)\n",
      "60 300.0 False (64, 64)\n",
      "61 305.0 False (64, 64)\n",
      "Step 2.2 Saving into test dataset\n",
      "output/testset_E06290634162_subjectawake_1638677765467.mp3_8000_4.1_None_64_0_None_512_512.pkl\n",
      "datafile =  output/testset_E06290634162_subjectawake_1638677765467.mp3_8000_4.1_None_64_0_None_512_512.pkl\n",
      "class = cry, len = 1\n",
      "class = nocry, len = 0\n",
      "dataset test len = 0\n",
      "Found no cry in video  Self/10Cry/E06290634162_subjectawake_1638677765467.mp4\n",
      "Step 1.1: Extracting audio file from video file Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4\n",
      "video_file =  Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4\n",
      "Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4  already exists\n",
      "Step 1.2: Read audio file Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4 and resample to 8000\n",
      "Step 2.1: Generating log-mel-spectrogram\n",
      "Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4 7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4\n",
      "170.04675 4.1 5.0 34\n",
      "0 0.0 False (64, 64)\n",
      "1 5.0 False (64, 64)\n",
      "2 10.0 False (64, 64)\n",
      "3 15.0 False (64, 64)\n",
      "4 20.0 False (64, 64)\n",
      "5 25.0 False (64, 64)\n",
      "6 30.0 False (64, 64)\n",
      "7 35.0 False (64, 64)\n",
      "8 40.0 False (64, 64)\n",
      "9 45.0 True (64, 64)\n",
      "10 50.0 True (64, 64)\n",
      "11 55.0 True (64, 64)\n",
      "12 60.0 True (64, 64)\n",
      "13 65.0 True (64, 64)\n",
      "14 70.0 False (64, 64)\n",
      "15 75.0 True (64, 64)\n",
      "16 80.0 False (64, 64)\n",
      "17 85.0 False (64, 64)\n",
      "18 90.0 False (64, 64)\n",
      "19 95.0 False (64, 64)\n",
      "20 100.0 False (64, 64)\n",
      "21 105.0 False (64, 64)\n",
      "22 110.0 False (64, 64)\n",
      "23 115.0 False (64, 64)\n",
      "24 120.0 False (64, 64)\n",
      "25 125.0 False (64, 64)\n",
      "26 130.0 False (64, 64)\n",
      "27 135.0 False (64, 64)\n",
      "28 140.0 False (64, 64)\n",
      "29 145.0 False (64, 64)\n",
      "30 150.0 False (64, 64)\n",
      "31 155.0 False (64, 64)\n",
      "32 160.0 False (64, 64)\n",
      "33 165.0 False (64, 64)\n",
      "Step 2.2 Saving into test dataset\n",
      "output/testset_7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_8000_4.1_None_64_0_None_512_512.pkl\n",
      "datafile =  output/testset_7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_8000_4.1_None_64_0_None_512_512.pkl\n",
      "class = cry, len = 1\n",
      "class = nocry, len = 0\n",
      "dataset test len = 6\n",
      "Step 3: Running cry detection CNN model\n",
      "Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4\n",
      "[(0.55433553, 9, True, True), (0.021777099, 10, True, False), (0.20367981, 11, True, False), (0.07290127, 12, True, False), (0.48973754, 13, True, False), (0.21996099, 15, True, False)]\n",
      "Step 4: Annotating the video file\n",
      "[((45.0, 50.0), 'Cry')]\n",
      "Moviepy - Building video Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_crydetected.MP4.\n",
      "MoviePy - Writing audio in 7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_crydetectedTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_crydetected.MP4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready Self/10Cry/7376bd92-3ccd-4672-bf08-2bb5dd992b30.MP4_crydetected.MP4\n"
     ]
    }
   ],
   "source": [
    "for v in video_files: \n",
    "    video_file = video_dir+v \n",
    "    test_cry_detection_on_video(video_file,sr=sr,segment=segment,pre_emphasis=pre_emphasis,\n",
    "                                n_mels=n_mels,fmin=fmin,fmax=fmax,n_fft=n_fft,hop_length=hop_length,\n",
    "                                VAD_on = VAD_on)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6ef9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:baby_cry] *",
   "language": "python",
   "name": "conda-env-baby_cry-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
